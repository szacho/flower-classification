{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version history\n",
    "- V12 - (ENet B7 + DenseNet, best_alpha=0.5) + AugMix + part of Oxford Dataset **(0.96564)**\n",
    "- V15 - (ENet B7 + DenseNet, best_alpha=0.44) + AugMix + part of Oxford Dataset (0.96194)\n",
    "- V16 - (ENet B7 + ResNet152V2, best_alpha=0.5) + AugMix + part of Oxford Dataset (0.96013)\n",
    "- V17 - (V12), EPOCHS 20-> 25 (0.96404)\n",
    "- V19 - (V12), ENet B7 weights -> noisy-student, no normalization, with validation (0.95663)\n",
    "- V20 - (V19) with normalization and validation (0.95331)\n",
    "- V21 - (V19) no validation **(0.96445)**\n",
    "- V22 - (V21), ENet B7 weights -> imagenet, remove custom params, 30 epochs (0.96375)\n",
    "- V23 - (V22), Enet for 25 epochs, DenseNet for 20 epochs, kick up augmix level and add normalization (0.96170)\n",
    "\n",
    "\n",
    "IDEAS: grid mask + cutmix ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "def measure_time(exec_history, event):\n",
    "    exec_history.append([event, time.time()-start_time])\n",
    "    return exec_history\n",
    "\n",
    "exec_history = measure_time([], \"Start notebook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import math, re, os\n",
    "import tensorflow as tf, tensorflow.keras.backend as K\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import efficientnet.tfkeras as efn\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path('flower-classification-with-tpus') # you can list the bucket with \"!gsutil ls $GCS_DS_PATH\"\n",
    "OXFORD_PATH = KaggleDatasets().get_gcs_path('oxford-102-for-tpu-competition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [512, 512] # at this size, a GPU will run out of memory. Use the TPU\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "\n",
    "GCS_PATH_SELECT = { # available image sizes\n",
    "    192: GCS_DS_PATH + '/tfrecords-jpeg-192x192',\n",
    "    224: GCS_DS_PATH + '/tfrecords-jpeg-224x224',\n",
    "    331: GCS_DS_PATH + '/tfrecords-jpeg-331x331',\n",
    "    512: GCS_DS_PATH + '/tfrecords-jpeg-512x512'\n",
    "}\n",
    "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
    "\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\n",
    "VALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\n",
    "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec') # predictions on this dataset should be submitted for the competition\n",
    "OXFORD_FILENAMES = tf.io.gfile.glob(OXFORD_PATH + '/*.tfrec')\n",
    "\n",
    "# watch out for overfitting!\n",
    "SKIP_VALIDATION = True\n",
    "if SKIP_VALIDATION:\n",
    "    TRAINING_FILENAMES = TRAINING_FILENAMES + VALIDATION_FILENAMES + OXFORD_FILENAMES\n",
    "\n",
    "\n",
    "CLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n",
    "           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n",
    "           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n",
    "           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n",
    "           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n",
    "           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n",
    "           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n",
    "           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n",
    "           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n",
    "           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n",
    "           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                                                                                                                                               # 100 - 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Import libraries, initilialize TPU and config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
    "    return image\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['class'], tf.int32)\n",
    "    return image, label # returns a dataset of (image, label) pairs\n",
    "\n",
    "def read_unlabeled_tfrecord(example):\n",
    "    UNLABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n",
    "        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    idnum = example['id']\n",
    "    return image, idnum # returns a dataset of image(s)\n",
    "\n",
    "def load_dataset(filenames, labeled = True, ordered = False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n",
    "    \n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "        \n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # use data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n",
    "    return dataset\n",
    "\n",
    "def get_training_dataset(augment=True, shuff=True):\n",
    "    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n",
    "    \n",
    "    if augment: \n",
    "        dataset = dataset.map(random_flip, num_parallel_calls=AUTO)\n",
    "        dataset = dataset.map(augmix, num_parallel_calls=AUTO)\n",
    "        \n",
    "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
    "    if shuff:\n",
    "        dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(ordered=False):\n",
    "    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE) # slighly faster with fixed tensor sizes\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_test_dataset(ordered=False):\n",
    "    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and matplotlib defaults\n",
    "np.set_printoptions(threshold=15, linewidth=80)\n",
    "\n",
    "def batch_to_numpy_images_and_labels(data):\n",
    "    images, labels = data\n",
    "    numpy_images = images.numpy()\n",
    "    numpy_labels = labels.numpy()\n",
    "    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n",
    "        numpy_labels = [None for _ in enumerate(numpy_images)]\n",
    "    # If no labels, only image IDs, return None for labels (this is the case for test data)\n",
    "    return numpy_images, numpy_labels\n",
    "\n",
    "def title_from_label_and_target(label, correct_label):\n",
    "    if correct_label is None:\n",
    "        return CLASSES[label], True\n",
    "    correct = (label == correct_label)\n",
    "    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n",
    "                                CLASSES[correct_label] if not correct else ''), correct\n",
    "\n",
    "def display_one_flower(image, title, subplot, red=False, titlesize=16):\n",
    "    plt.subplot(*subplot)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    if len(title) > 0:\n",
    "        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
    "    return (subplot[0], subplot[1], subplot[2]+1)\n",
    "    \n",
    "def display_batch_of_images(databatch, predictions=None, figsize  = 13.0):\n",
    "    \"\"\"This will work with:\n",
    "    display_batch_of_images(images)\n",
    "    display_batch_of_images(images, predictions)\n",
    "    display_batch_of_images((images, labels))\n",
    "    display_batch_of_images((images, labels), predictions)\n",
    "    \"\"\"\n",
    "    # data\n",
    "    images, labels = batch_to_numpy_images_and_labels(databatch)\n",
    "    if labels is None:\n",
    "        labels = [None for _ in enumerate(images)]\n",
    "        \n",
    "    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n",
    "    rows = int(math.sqrt(len(images)))\n",
    "    cols = len(images)//rows\n",
    "        \n",
    "    # size and spacing\n",
    "    FIGSIZE =  figsize\n",
    "    SPACING = 0.1\n",
    "    subplot=(rows,cols,1)\n",
    "    if rows < cols:\n",
    "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
    "    else:\n",
    "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
    "    \n",
    "    # display\n",
    "    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n",
    "        title = '' if label is None else CLASSES[label]\n",
    "        correct = True\n",
    "        if predictions is not None:\n",
    "            title, correct = title_from_label_and_target(predictions[i], label)\n",
    "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n",
    "        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n",
    "    \n",
    "    #layout\n",
    "    plt.tight_layout()\n",
    "    if label is None and predictions is None:\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    else:\n",
    "        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
    "    plt.show()\n",
    "\n",
    "def display_confusion_matrix(cmat, score, precision, recall):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    ax = plt.gca()\n",
    "    ax.matshow(cmat, cmap='Reds')\n",
    "    ax.set_xticks(range(len(CLASSES)))\n",
    "    ax.set_xticklabels(CLASSES, fontdict={'fontsize': 7})\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n",
    "    ax.set_yticks(range(len(CLASSES)))\n",
    "    ax.set_yticklabels(CLASSES, fontdict={'fontsize': 7})\n",
    "    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    titlestring = \"\"\n",
    "    if score is not None:\n",
    "        titlestring += 'f1 = {:.3f} '.format(score)\n",
    "    if precision is not None:\n",
    "        titlestring += '\\nprecision = {:.3f} '.format(precision)\n",
    "    if recall is not None:\n",
    "        titlestring += '\\nrecall = {:.3f} '.format(recall)\n",
    "    if len(titlestring) > 0:\n",
    "        ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n",
    "    plt.show()\n",
    "    \n",
    "def display_training_curves(training, validation, title, subplot):\n",
    "    if subplot%10==1: # set up the subplots on the first call\n",
    "        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
    "        plt.tight_layout()\n",
    "    ax = plt.subplot(subplot)\n",
    "    ax.set_facecolor('#F8F8F8')\n",
    "    ax.plot(training)\n",
    "    ax.plot(validation)\n",
    "    ax.set_title('model '+ title)\n",
    "    ax.set_ylabel(title)\n",
    "    #ax.set_ylim(0.28,1.05)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'valid.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_parameter(level, maxval):\n",
    "    return tf.cast(level * maxval / 10, tf.int32)\n",
    "\n",
    "def float_parameter(level, maxval):\n",
    "    return tf.cast((level) * maxval / 10., tf.float32)\n",
    "\n",
    "def sample_level(n):\n",
    "    return tf.random.uniform(shape=[1], minval=0.1, maxval=n, dtype=tf.float32)\n",
    "    # return np.random.uniform(low=0.1, high=n)\n",
    "\n",
    "def affine_transform(image, transform_matrix):\n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    DIM = IMAGE_SIZE[0]\n",
    "    XDIM = DIM%2 #fix for size 331\n",
    "    \n",
    "    x = tf.repeat(tf.range(DIM//2,-DIM//2,-1), DIM)\n",
    "    y = tf.tile(tf.range(-DIM//2,DIM//2), [DIM])\n",
    "    z = tf.ones([DIM*DIM], dtype='int32')\n",
    "    idx = tf.stack([x, y, z])\n",
    "    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(transform_matrix, tf.cast(idx, dtype='float32'))\n",
    "    idx2 = K.cast(idx2, dtype='int32')\n",
    "    idx2 = K.clip(idx2, -DIM//2+XDIM+1, DIM//2)\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack([DIM//2-idx2[0,], DIM//2-1+idx2[1,]])\n",
    "    d = tf.gather_nd(image, tf.transpose(idx3))\n",
    "    return tf.reshape(d,[DIM,DIM,3])\n",
    "\n",
    "def rotate(image, level):\n",
    "    degrees = float_parameter(sample_level(level), 30)\n",
    "    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n",
    "    degrees = tf.cond(rand_var > 0.5, lambda: degrees, lambda: -degrees)\n",
    "    # if  > 0.5:\n",
    "    #     degrees = -degrees\n",
    "\n",
    "    angle = math.pi*degrees/180 # convert degrees to radians\n",
    "    # angle = tf.constant([angle], dtype='float32')\n",
    "    angle = tf.cast(angle, tf.float32)\n",
    "    # define rotation matrix\n",
    "    c1 = tf.math.cos(angle)\n",
    "    s1 = tf.math.sin(angle)\n",
    "    one = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    rotation_matrix = tf.reshape(tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one], axis=0), [3,3])\n",
    "\n",
    "    transformed = affine_transform(image, rotation_matrix)\n",
    "    return transformed\n",
    "\n",
    "def translate_x(image, level):\n",
    "    lvl = int_parameter(sample_level(level), IMAGE_SIZE[0] / 3)\n",
    "    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n",
    "    lvl = tf.cond(rand_var > 0.5, lambda: lvl, lambda: -lvl)\n",
    "\n",
    "    one = tf.constant([1], dtype='float32')\n",
    "    zero = tf.constant([0], dtype='float32')\n",
    "    lvl = tf.cast(lvl, tf.float32)\n",
    "    translate_x_matrix = tf.reshape(tf.concat([one,zero,zero, zero,one,lvl, zero,zero,one], axis=0), [3,3])\n",
    "\n",
    "    transformed = affine_transform(image, translate_x_matrix)\n",
    "    return transformed\n",
    "\n",
    "def translate_y(image, level):\n",
    "    lvl = int_parameter(sample_level(level), IMAGE_SIZE[0] / 3)\n",
    "    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n",
    "    lvl = tf.cond(rand_var > 0.5, lambda: lvl, lambda: -lvl)\n",
    "\n",
    "    one = tf.constant([1], dtype='float32')\n",
    "    zero = tf.constant([0], dtype='float32')\n",
    "    lvl = tf.cast(lvl, tf.float32)\n",
    "    translate_y_matrix = tf.reshape(tf.concat([one,zero,lvl, zero,one,zero, zero,zero,one], axis=0), [3,3])\n",
    "\n",
    "    transformed = affine_transform(image, translate_y_matrix)\n",
    "    return transformed\n",
    "\n",
    "def shear_x(image, level):\n",
    "    lvl = float_parameter(sample_level(level), 0.3)\n",
    "    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n",
    "    lvl = tf.cond(rand_var > 0.5, lambda: lvl, lambda: -lvl)\n",
    "\n",
    "    one = tf.constant([1], dtype='float32')\n",
    "    zero = tf.constant([0], dtype='float32')\n",
    "    s2 = tf.math.sin(lvl)\n",
    "    shear_x_matrix = tf.reshape(tf.concat([one,s2,zero, zero,one,zero, zero,zero,one],axis=0), [3,3])   \n",
    "\n",
    "    transformed = affine_transform(image, shear_x_matrix)\n",
    "    return transformed\n",
    "\n",
    "def shear_y(image, level):\n",
    "    lvl = float_parameter(sample_level(level), 0.3)\n",
    "    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n",
    "    lvl = tf.cond(rand_var > 0.5, lambda: lvl, lambda: -lvl)\n",
    "\n",
    "    one = tf.constant([1], dtype='float32')\n",
    "    zero = tf.constant([0], dtype='float32')\n",
    "    c2 = tf.math.cos(lvl)\n",
    "    shear_y_matrix = tf.reshape(tf.concat([one,zero,zero, zero,c2,zero, zero,zero,one],axis=0), [3,3])   \n",
    "    \n",
    "    transformed = affine_transform(image, shear_y_matrix)\n",
    "    return transformed\n",
    "\n",
    "def solarize(image, level):\n",
    "    # For each pixel in the image, select the pixel\n",
    "    # if the value is less than the threshold.\n",
    "    # Otherwise, subtract 255 from the pixel.\n",
    "    threshold = float_parameter(sample_level(level), 1)\n",
    "    return tf.where(image < threshold, image, 1 - image)\n",
    "\n",
    "def solarize_add(image, level):\n",
    "    # For each pixel in the image less than threshold\n",
    "    # we add 'addition' amount to it and then clip the\n",
    "    # pixel value to be between 0 and 255. The value\n",
    "    # of 'addition' is between -128 and 128.\n",
    "    threshold = float_parameter(sample_level(level), 1)\n",
    "    addition = float_parameter(sample_level(level), 0.5)\n",
    "    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n",
    "    addition = tf.cond(rand_var > 0.5, lambda: addition, lambda: -addition)\n",
    "\n",
    "    added_image = tf.cast(image, tf.float32) + addition\n",
    "    added_image = tf.cast(tf.clip_by_value(added_image, 0, 1), tf.float32)\n",
    "    return tf.where(image < threshold, added_image, image)\n",
    "\n",
    "def posterize(image, level):\n",
    "    lvl = int_parameter(sample_level(level), 8)\n",
    "    shift = 8 - lvl\n",
    "    shift = tf.cast(shift, tf.uint8)\n",
    "    image = tf.cast(tf.math.scalar_mul(255, image), tf.uint8)\n",
    "    image = tf.bitwise.left_shift(tf.bitwise.right_shift(image, shift), shift)\n",
    "    return tf.cast(tf.clip_by_value(tf.math.divide(image, 255), 0, 1), tf.float32)\n",
    "\n",
    "def autocontrast(image, _): # add cutoff ?\n",
    "    image = tf.cast(tf.math.scalar_mul(255, image), tf.uint8)\n",
    "\n",
    "    def scale_channel(image):\n",
    "        # A possibly cheaper version can be done using cumsum/unique_with_counts\n",
    "        # over the histogram values, rather than iterating over the entire image.\n",
    "        # to compute mins and maxes.\n",
    "        lo = tf.cast(tf.reduce_min(image), tf.float32)\n",
    "        hi = tf.cast(tf.reduce_max(image), tf.float32)\n",
    "\n",
    "        # Scale the image, making the lowest value 0 and the highest value 255.\n",
    "        def scale_values(im):\n",
    "            scale = 255.0 / (hi - lo)\n",
    "            offset = -lo * scale\n",
    "            im = tf.cast(im, tf.float32) * scale + offset\n",
    "            im = tf.clip_by_value(im, 0.0, 255.0)\n",
    "            return tf.cast(im, tf.uint8)\n",
    "\n",
    "        result = tf.cond(hi > lo, lambda: scale_values(image), lambda: image)\n",
    "        return result\n",
    "\n",
    "    # Assumes RGB for now.  Scales each channel independently\n",
    "    # and then stacks the result.\n",
    "    s1 = scale_channel(image[:, :, 0])\n",
    "    s2 = scale_channel(image[:, :, 1])\n",
    "    s3 = scale_channel(image[:, :, 2])\n",
    "    image = tf.stack([s1, s2, s3], 2)\n",
    "    return tf.cast(tf.clip_by_value(tf.math.divide(image, 255), 0, 1), tf.float32)\n",
    "\n",
    "def equalize(image, _):\n",
    "    image = tf.cast(tf.math.scalar_mul(255, image), tf.uint8)\n",
    "\n",
    "    def scale_channel(im, c):\n",
    "        im = tf.cast(im[:, :, c], tf.int32)\n",
    "        # Compute the histogram of the image channel.\n",
    "        histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n",
    "        # For the purposes of computing the step, filter out the nonzeros.\n",
    "        nonzero = tf.where(tf.not_equal(histo, 0))\n",
    "        nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n",
    "        step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n",
    "\n",
    "        def build_lut(histo, step):\n",
    "            # Compute the cumulative sum, shifting by step // 2\n",
    "            # and then normalization by step.\n",
    "            lut = (tf.cumsum(histo) + (step // 2)) // step\n",
    "            # Shift lut, prepending with 0.\n",
    "            lut = tf.concat([[0], lut[:-1]], 0)\n",
    "            # Clip the counts to be in range.  This is done\n",
    "            # in the C code for image.point.\n",
    "            return tf.clip_by_value(lut, 0, 255)\n",
    "\n",
    "        # If step is zero, return the original image.  Otherwise, build\n",
    "        # lut from the full histogram and step and then index from it.\n",
    "        result = tf.cond(tf.equal(step, 0),\n",
    "                        lambda: im,\n",
    "                        lambda: tf.gather(build_lut(histo, step), im))\n",
    "\n",
    "        return tf.cast(result, tf.uint8)\n",
    "\n",
    "    # Assumes RGB for now.  Scales each channel independently\n",
    "    # and then stacks the result.\n",
    "    s1 = scale_channel(image, 0)\n",
    "    s2 = scale_channel(image, 1)\n",
    "    s3 = scale_channel(image, 2)\n",
    "    image = tf.stack([s1, s2, s3], 2)\n",
    "\n",
    "    return tf.cast(tf.clip_by_value(tf.math.divide(image, 255), 0, 1), tf.float32)\n",
    "\n",
    "\n",
    "def blend(image1, image2, factor):\n",
    "    if factor == 0.0:\n",
    "        return tf.convert_to_tensor(image1)\n",
    "    if factor == 1.0:\n",
    "        return tf.convert_to_tensor(image2)\n",
    "\n",
    "    image1 = tf.cast(image1, tf.float32)\n",
    "    image2 = tf.cast(image2, tf.float32)\n",
    "\n",
    "    difference = image2 - image1\n",
    "    scaled = factor * difference\n",
    "\n",
    "    # Do addition in float.\n",
    "    temp = tf.cast(image1, tf.float32) + scaled\n",
    "\n",
    "    # Interpolate\n",
    "    if factor > 0.0 and factor < 1.0:\n",
    "        # Interpolation means we always stay within 0 and 255.\n",
    "        return tf.cast(temp, tf.uint8)\n",
    "\n",
    "    # Extrapolate:\n",
    "    #\n",
    "    # We need to clip and then cast.\n",
    "    return tf.cast(tf.clip_by_value(temp, 0.0, 255.0), tf.uint8)\n",
    "\n",
    "def color(image, level):\n",
    "    factor = float_parameter(sample_level(level), 1.8) + 0.1\n",
    "    image = tf.cast(tf.math.scalar_mul(255, image), tf.uint8)\n",
    "    degenerate = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image))\n",
    "    blended = blend(degenerate, image, factor)\n",
    "    return tf.cast(tf.clip_by_value(tf.math.divide(blended, 255), 0, 1), tf.float32)\n",
    "\n",
    "def brightness(image, level):\n",
    "    delta = float_parameter(sample_level(level), 0.5) + 0.1\n",
    "    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n",
    "    delta = tf.cond(rand_var > 0.5, lambda: delta, lambda: -delta) \n",
    "    return tf.image.adjust_brightness(image, delta=delta)\n",
    "\n",
    "def contrast(image, level):\n",
    "    factor = float_parameter(sample_level(level), 1.8) + 0.1\n",
    "    factor = tf.reshape(factor, [])\n",
    "    rand_var = tf.random.uniform(shape=[], dtype=tf.float32)\n",
    "    factor = tf.cond(rand_var > 0.5, lambda: factor, lambda: 1.9 - factor  )\n",
    "\n",
    "    return tf.image.adjust_contrast(image, factor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AugMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = {'R': 0.3017198650030484, 'G': 0.41498675214167186, 'B': 0.4490430096189503}\n",
    "stds = {'R': 0.2209235100832713, 'G': 0.21134712855959467, 'B': 0.24709655695020577}\n",
    "\n",
    "def substract_means(image):\n",
    "    image = image - np.array([means['B'], means['G'], means['R']])\n",
    "    return image\n",
    "\n",
    "def normalize(image):\n",
    "    image = substract_means(image)\n",
    "    image = image / np.array([stds['B'], stds['G'], stds['R']])\n",
    "    return tf.clip_by_value(image, 0, 1)\n",
    "\n",
    "def apply_op(image, level, which):\n",
    "    # desperate solution, noobing in tensors\n",
    "    augmented = image\n",
    "    augmented = tf.cond(which == tf.constant([0], dtype=tf.int32), lambda: rotate(image, level), lambda: augmented)\n",
    "    augmented = tf.cond(which == tf.constant([1], dtype=tf.int32), lambda: translate_x(image, level), lambda: augmented)\n",
    "    augmented = tf.cond(which == tf.constant([2], dtype=tf.int32), lambda: translate_y(image, level), lambda: augmented)\n",
    "    augmented = tf.cond(which == tf.constant([3], dtype=tf.int32), lambda: shear_x(image, level), lambda: augmented)\n",
    "    augmented = tf.cond(which == tf.constant([4], dtype=tf.int32), lambda: shear_y(image, level), lambda: augmented)\n",
    "    augmented = tf.cond(which == tf.constant([5], dtype=tf.int32), lambda: solarize_add(image, level), lambda: augmented)\n",
    "    augmented = tf.cond(which == tf.constant([6], dtype=tf.int32), lambda: solarize(image, level), lambda: augmented)\n",
    "    augmented = tf.cond(which == tf.constant([7], dtype=tf.int32), lambda: posterize(image, level), lambda: augmented)\n",
    "    augmented = tf.cond(which == tf.constant([8], dtype=tf.int32), lambda: autocontrast(image, level), lambda: augmented)\n",
    "    augmented = tf.cond(which == tf.constant([9], dtype=tf.int32), lambda: equalize(image, level), lambda: augmented)\n",
    "    augmented = tf.cond(which == tf.constant([10], dtype=tf.int32), lambda: color(image, level), lambda: augmented)\n",
    "    augmented = tf.cond(which == tf.constant([11], dtype=tf.int32), lambda: contrast(image, level), lambda: augmented)\n",
    "    augmented = tf.cond(which == tf.constant([12], dtype=tf.int32), lambda: brightness(image, level), lambda: augmented)\n",
    "    return augmented\n",
    "\n",
    "def augmix(image, label):\n",
    "    severity = 7\n",
    "    width = 3\n",
    "    depth = -1\n",
    "    alpha = 1.\n",
    "\n",
    "    dir_dist = tfp.distributions.Dirichlet([alpha]*width)\n",
    "    ws = tf.cast(dir_dist.sample(), tf.float32)\n",
    "    beta_dist = tfp.distributions.Beta(alpha, alpha)\n",
    "    m = tf.cast(beta_dist.sample(), tf.float32)\n",
    "\n",
    "    mix = tf.zeros_like(image, dtype='float32')\n",
    "\n",
    "    def outer_loop_cond(i, depth, mix):\n",
    "        return tf.less(i, width)\n",
    "\n",
    "    def outer_loop_body(i, depth, mix):\n",
    "        image_aug = tf.identity(image)\n",
    "        depth = tf.cond(tf.greater(depth, 0), lambda: depth, lambda: tf.random.uniform(shape=[], minval=1, maxval=4, dtype=tf.int32))\n",
    "\n",
    "        def inner_loop_cond(j, image_aug):\n",
    "            return tf.less(j, depth)\n",
    "\n",
    "        def inner_loop_body(j, image_aug):\n",
    "            which = tf.random.uniform(shape=[], minval=0, maxval=3, dtype=tf.int32)\n",
    "            image_aug = apply_op(image_aug, severity, which)\n",
    "            j = tf.add(j, 1)\n",
    "            return j, image_aug\n",
    "        \n",
    "        j = tf.constant([0], dtype=tf.int32)\n",
    "        j, image_aug = tf.while_loop(inner_loop_cond, inner_loop_body, [j, image_aug])\n",
    "\n",
    "        # Preprocessing commutes since all coefficients are convex\n",
    "        wsi = tf.gather(ws, i)\n",
    "        mix = tf.add(mix, wsi*normalize(image_aug))\n",
    "        i = tf.add(i, 1)\n",
    "        return i, depth, mix\n",
    "\n",
    "    i = tf.constant([0], dtype=tf.int32)\n",
    "    i, depth, mix = tf.while_loop(outer_loop_cond, outer_loop_body, [i, depth, mix])\n",
    "    \n",
    "    mixed = tf.math.scalar_mul((1 - m), normalize(image)) + tf.math.scalar_mul(m, mix) # NORMALIZE\n",
    "    return tf.clip_by_value(mixed, 0, 1), label\n",
    "\n",
    "def random_flip(image, label):\n",
    "    transformed = tf.image.random_flip_left_right(image)\n",
    "    return transformed, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Initialize dataset, visualization and augmentation functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_me = False\n",
    "if show_me:\n",
    "    dataset = get_training_dataset(augment=True, shuff=False)\n",
    "    training_dataset = dataset.unbatch().batch(20)\n",
    "    train_batch = iter(training_dataset)\n",
    "    display_batch_of_images(next(train_batch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate schedule: 0.0001 to 0.0004 to 1.29e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Learning rate schedule for TPU, GPU and CPU.\n",
    "# Using an LR ramp up because fine-tuning a pre-trained model.\n",
    "# Starting with a high LR would break the pre-trained weights.\n",
    "\n",
    "LR_START = 0.0001\n",
    "LR_MAX = 0.00005 * strategy.num_replicas_in_sync\n",
    "LR_MIN = 0.00001\n",
    "LR_RAMPUP_EPOCHS = 3\n",
    "LR_SUSTAIN_EPOCHS = 4\n",
    "LR_EXP_DECAY = .75\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return lr\n",
    "    \n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "\n",
    "rng = [i for i in range(25 if EPOCHS<25 else EPOCHS)]\n",
    "y = [lrfn(x) for x in rng]\n",
    "plt.plot(rng, y)\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 19183 training images, 3712 validation images, 7382 unlabeled test images\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
    "NUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\n",
    "NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "\n",
    "print('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Begin downloading ENet B7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet B7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b7_noisy-student_notop.h5\n",
      "258072576/258068648 [==============================] - 10s 0us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b7 (Model)      (None, 16, 16, 2560)      64097680  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 104)               266344    \n",
      "=================================================================\n",
      "Total params: 64,364,024\n",
      "Trainable params: 64,053,304\n",
      "Non-trainable params: 310,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# efficient net\n",
    "with strategy.scope():\n",
    "    enet = efn.EfficientNetB7(\n",
    "        input_shape=(*IMAGE_SIZE, 3),\n",
    "        weights='noisy-student',\n",
    "        include_top=False\n",
    "    )\n",
    "    enet.trainable = True\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        enet,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n",
    "    ])\n",
    "        \n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Finish downloading ENet B7, start training ENet B7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 149 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/25\n",
      "149/149 [==============================] - 484s 3s/step - loss: 3.5881 - sparse_categorical_accuracy: 0.2307\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 2/25\n",
      "149/149 [==============================] - 183s 1s/step - loss: 1.4965 - sparse_categorical_accuracy: 0.6456\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00030000000000000003.\n",
      "Epoch 3/25\n",
      "149/149 [==============================] - 174s 1s/step - loss: 0.8291 - sparse_categorical_accuracy: 0.7852\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 4/25\n",
      "149/149 [==============================] - 163s 1s/step - loss: 0.6155 - sparse_categorical_accuracy: 0.8268\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 5/25\n",
      "149/149 [==============================] - 163s 1s/step - loss: 0.4830 - sparse_categorical_accuracy: 0.8654\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 6/25\n",
      "149/149 [==============================] - 164s 1s/step - loss: 0.3747 - sparse_categorical_accuracy: 0.8884\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 7/25\n",
      "149/149 [==============================] - 167s 1s/step - loss: 0.3155 - sparse_categorical_accuracy: 0.9073\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 8/25\n",
      "149/149 [==============================] - 164s 1s/step - loss: 0.2726 - sparse_categorical_accuracy: 0.9274\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00030250000000000003.\n",
      "Epoch 9/25\n",
      "149/149 [==============================] - 166s 1s/step - loss: 0.2097 - sparse_categorical_accuracy: 0.9379\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.000229375.\n",
      "Epoch 10/25\n",
      "149/149 [==============================] - 166s 1s/step - loss: 0.1577 - sparse_categorical_accuracy: 0.9555\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00017453125.\n",
      "Epoch 11/25\n",
      "149/149 [==============================] - 168s 1s/step - loss: 0.1239 - sparse_categorical_accuracy: 0.9643\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0001333984375.\n",
      "Epoch 12/25\n",
      "149/149 [==============================] - 170s 1s/step - loss: 0.1043 - sparse_categorical_accuracy: 0.9652\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00010254882812499999.\n",
      "Epoch 13/25\n",
      "149/149 [==============================] - 166s 1s/step - loss: 0.0833 - sparse_categorical_accuracy: 0.9748\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 7.941162109375e-05.\n",
      "Epoch 14/25\n",
      "149/149 [==============================] - 164s 1s/step - loss: 0.0782 - sparse_categorical_accuracy: 0.9740\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 6.20587158203125e-05.\n",
      "Epoch 15/25\n",
      "149/149 [==============================] - 168s 1s/step - loss: 0.0720 - sparse_categorical_accuracy: 0.9773\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 4.9044036865234376e-05.\n",
      "Epoch 16/25\n",
      "149/149 [==============================] - 169s 1s/step - loss: 0.0643 - sparse_categorical_accuracy: 0.9778\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 3.928302764892578e-05.\n",
      "Epoch 17/25\n",
      "149/149 [==============================] - 170s 1s/step - loss: 0.0567 - sparse_categorical_accuracy: 0.9866\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 3.196227073669434e-05.\n",
      "Epoch 18/25\n",
      "149/149 [==============================] - 164s 1s/step - loss: 0.0526 - sparse_categorical_accuracy: 0.9849\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 2.6471703052520753e-05.\n",
      "Epoch 19/25\n",
      "149/149 [==============================] - 166s 1s/step - loss: 0.0531 - sparse_categorical_accuracy: 0.9853\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 2.2353777289390565e-05.\n",
      "Epoch 20/25\n",
      "149/149 [==============================] - 167s 1s/step - loss: 0.0531 - sparse_categorical_accuracy: 0.9862\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 1.9265332967042924e-05.\n",
      "Epoch 21/25\n",
      "149/149 [==============================] - 168s 1s/step - loss: 0.0497 - sparse_categorical_accuracy: 0.9895\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 1.6948999725282194e-05.\n",
      "Epoch 22/25\n",
      "149/149 [==============================] - 170s 1s/step - loss: 0.0514 - sparse_categorical_accuracy: 0.9866\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 1.5211749793961645e-05.\n",
      "Epoch 23/25\n",
      "149/149 [==============================] - 165s 1s/step - loss: 0.0460 - sparse_categorical_accuracy: 0.9849\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 1.3908812345471235e-05.\n",
      "Epoch 24/25\n",
      "149/149 [==============================] - 165s 1s/step - loss: 0.0439 - sparse_categorical_accuracy: 0.9874\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.2931609259103426e-05.\n",
      "Epoch 25/25\n",
      "149/149 [==============================] - 167s 1s/step - loss: 0.0440 - sparse_categorical_accuracy: 0.9857\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    get_training_dataset(), \n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[lr_callback],\n",
    "    validation_data=None if SKIP_VALIDATION else get_validation_dataset()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Finish training ENet B7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_VALIDATION:\n",
    "    display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n",
    "    display_training_curves(history.history['sparse_categorical_accuracy'], history.history['val_sparse_categorical_accuracy'], 'accuracy', 212)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Begin downloading DenseNet 201\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "74842112/74836368 [==============================] - 3s 0us/step\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "densenet201 (Model)          (None, 16, 16, 1920)      18321984  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 104)               199784    \n",
      "=================================================================\n",
      "Total params: 18,521,768\n",
      "Trainable params: 18,292,712\n",
      "Non-trainable params: 229,056\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    dnet = DenseNet201(\n",
    "        input_shape=(*IMAGE_SIZE, 3),\n",
    "        weights='imagenet',\n",
    "        include_top=False\n",
    "    )\n",
    "    dnet.trainable = True\n",
    "\n",
    "    model2 = tf.keras.Sequential([\n",
    "        dnet,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n",
    "    ])\n",
    "        \n",
    "model2.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "model2.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Finish downloading DenseNet 201, start training DenseNet 201\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 149 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/25\n",
      "149/149 [==============================] - 392s 3s/step - loss: 2.2392 - sparse_categorical_accuracy: 0.5159\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 2/25\n",
      "149/149 [==============================] - 139s 935ms/step - loss: 0.9745 - sparse_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00030000000000000003.\n",
      "Epoch 3/25\n",
      "149/149 [==============================] - 141s 943ms/step - loss: 0.6703 - sparse_categorical_accuracy: 0.8284\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 4/25\n",
      "149/149 [==============================] - 144s 966ms/step - loss: 0.5520 - sparse_categorical_accuracy: 0.8523\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 5/25\n",
      "149/149 [==============================] - 146s 980ms/step - loss: 0.4306 - sparse_categorical_accuracy: 0.8943\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 6/25\n",
      "149/149 [==============================] - 144s 965ms/step - loss: 0.3546 - sparse_categorical_accuracy: 0.9023\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 7/25\n",
      "149/149 [==============================] - 148s 993ms/step - loss: 0.2916 - sparse_categorical_accuracy: 0.9178\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 8/25\n",
      "149/149 [==============================] - 145s 974ms/step - loss: 0.2576 - sparse_categorical_accuracy: 0.9404\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00030250000000000003.\n",
      "Epoch 9/25\n",
      "149/149 [==============================] - 146s 981ms/step - loss: 0.1744 - sparse_categorical_accuracy: 0.9505\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.000229375.\n",
      "Epoch 10/25\n",
      "149/149 [==============================] - 148s 992ms/step - loss: 0.1196 - sparse_categorical_accuracy: 0.9732\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00017453125.\n",
      "Epoch 11/25\n",
      "149/149 [==============================] - 149s 1s/step - loss: 0.0724 - sparse_categorical_accuracy: 0.9820\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0001333984375.\n",
      "Epoch 12/25\n",
      "149/149 [==============================] - 146s 982ms/step - loss: 0.0549 - sparse_categorical_accuracy: 0.9836\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00010254882812499999.\n",
      "Epoch 13/25\n",
      "149/149 [==============================] - 150s 1s/step - loss: 0.0441 - sparse_categorical_accuracy: 0.9904\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 7.941162109375e-05.\n",
      "Epoch 14/25\n",
      "149/149 [==============================] - 151s 1s/step - loss: 0.0334 - sparse_categorical_accuracy: 0.9954\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 6.20587158203125e-05.\n",
      "Epoch 15/25\n",
      "149/149 [==============================] - 151s 1s/step - loss: 0.0282 - sparse_categorical_accuracy: 0.9929\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 4.9044036865234376e-05.\n",
      "Epoch 16/25\n",
      "149/149 [==============================] - 151s 1s/step - loss: 0.0254 - sparse_categorical_accuracy: 0.9975\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 3.928302764892578e-05.\n",
      "Epoch 17/25\n",
      "149/149 [==============================] - 153s 1s/step - loss: 0.0226 - sparse_categorical_accuracy: 0.9954\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 3.196227073669434e-05.\n",
      "Epoch 18/25\n",
      "149/149 [==============================] - 153s 1s/step - loss: 0.0216 - sparse_categorical_accuracy: 0.9966\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 2.6471703052520753e-05.\n",
      "Epoch 19/25\n",
      "149/149 [==============================] - 151s 1s/step - loss: 0.0190 - sparse_categorical_accuracy: 0.9975\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 2.2353777289390565e-05.\n",
      "Epoch 20/25\n",
      "149/149 [==============================] - 154s 1s/step - loss: 0.0183 - sparse_categorical_accuracy: 0.9971\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 1.9265332967042924e-05.\n",
      "Epoch 21/25\n",
      "149/149 [==============================] - 151s 1s/step - loss: 0.0178 - sparse_categorical_accuracy: 0.9971\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 1.6948999725282194e-05.\n",
      "Epoch 22/25\n",
      "149/149 [==============================] - 152s 1s/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9971\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 1.5211749793961645e-05.\n",
      "Epoch 23/25\n",
      "149/149 [==============================] - 151s 1s/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9966\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 1.3908812345471235e-05.\n",
      "Epoch 24/25\n",
      "149/149 [==============================] - 153s 1s/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9979\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.2931609259103426e-05.\n",
      "Epoch 25/25\n",
      "149/149 [==============================] - 153s 1s/step - loss: 0.0155 - sparse_categorical_accuracy: 0.9966\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(\n",
    "    get_training_dataset(), \n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs=EPOCHS, \n",
    "    callbacks=[lr_callback],\n",
    "    validation_data=None if SKIP_VALIDATION else get_validation_dataset()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Finish training DenseNet 201\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_VALIDATION:\n",
    "    display_training_curves(history2.history['loss'], history2.history['val_loss'], 'loss', 211)\n",
    "    display_training_curves(history2.history['sparse_categorical_accuracy'], history2.history['val_sparse_categorical_accuracy'], 'accuracy', 212)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_VALIDATION:\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    def normalize_testset(image, id_):\n",
    "        image = substract_means(image)\n",
    "        image = image / np.array([stds['B'], stds['G'], stds['R']])\n",
    "        return tf.clip_by_value(image, 0, 1), id_\n",
    "\n",
    "    def show_report(model):\n",
    "        val_dataset = load_dataset(VALIDATION_FILENAMES, labeled=False, ordered=True)\n",
    "\n",
    "        val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "        val_dataset = val_dataset.prefetch(AUTO)\n",
    "        val_images_ds = val_dataset.map(normalize_testset)\n",
    "        val_images_ds = val_dataset.map(lambda image, idnum: image)\n",
    "\n",
    "        val_dataset2 = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=True)\n",
    "        val_labels_ds = [ label.numpy() for image, label in val_dataset2]\n",
    "        # val_names_ds = val_labels_ds.map(lambda idnum: CLASSES[idnum])\n",
    "\n",
    "        print(\"[INFO] evaluating...\")\n",
    "        probabilities = model.predict(val_images_ds)\n",
    "        predictions = np.argmax(probabilities, axis=-1)\n",
    "        print(classification_report(val_labels_ds, predictions, target_names=CLASSES))\n",
    "\n",
    "        # # compute the raw accuracy with extra precision\n",
    "        acc = accuracy_score(val_labels_ds, predictions)\n",
    "        print(\"[INFO] score: {}\".format(acc))\n",
    "\n",
    "    show_report(model)\n",
    "    show_report(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models\n",
    "# model.save('effModel.h5')\n",
    "# model2.save('denseModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_history = measure_time(exec_history, \"Start calculating predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Computing predictions for alpha = 0.5\n",
      "[ 67  28  83 ...  53 102  62]\n",
      "[INFO] Generating submission.csv file...\n"
     ]
    }
   ],
   "source": [
    "def normalize_testset(image, id_):\n",
    "    image = substract_means(image)\n",
    "    image = image / np.array([stds['B'], stds['G'], stds['R']])\n",
    "    return tf.clip_by_value(image, 0, 1), id_\n",
    "\n",
    "test_dataset = get_test_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and ids, order matters.\n",
    "test_dataset = test_dataset.map(normalize_testset, num_parallel_calls=AUTO)\n",
    "\n",
    "def get_predictions(alpha):\n",
    "    print('[INFO] Computing predictions for alpha = {}'.format(alpha))\n",
    "    test_images_ds = test_dataset.map(lambda image, idnum: image)\n",
    "    probabilities = alpha*model.predict(test_images_ds) + (1-alpha)*model2.predict(test_images_ds)\n",
    "    predictions = np.argmax(probabilities, axis=-1)\n",
    "    print(predictions)\n",
    "\n",
    "    print('[INFO] Generating submission.csv file...')\n",
    "    test_ids_ds = test_dataset.map(lambda image, idnum: idnum).unbatch()\n",
    "    test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n",
    "    np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\n",
    "    \n",
    "get_predictions(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0.00s - Start notebook\n",
      "  26.39s - Import libraries, initilialize TPU and config\n",
      "  26.63s - Initialize dataset, visualization and augmentation functions\n",
      "  26.88s - Begin downloading ENet B7\n",
      " 113.46s - Finish downloading ENet B7, start training ENet B7\n",
      "4624.13s - Finish training ENet B7\n",
      "4624.15s - Begin downloading DenseNet 201\n",
      "4696.11s - Finish downloading DenseNet 201, start training DenseNet 201\n",
      "8659.57s - Finish training DenseNet 201\n",
      "8659.61s - Start calculating predictions\n",
      "9065.30s - Finish calculating predictions\n"
     ]
    }
   ],
   "source": [
    "exec_history = measure_time(exec_history, \"Finish calculating predictions\")\n",
    "\n",
    "for event in exec_history:\n",
    "    print(f'{event[1]:7.2f}s - {event[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
